{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7031838b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:37:06.850448Z",
     "iopub.status.busy": "2022-01-21T21:37:06.848717Z",
     "iopub.status.idle": "2022-01-21T21:37:17.746598Z",
     "shell.execute_reply": "2022-01-21T21:37:17.745976Z",
     "shell.execute_reply.started": "2022-01-21T21:21:36.460757Z"
    },
    "papermill": {
     "duration": 10.913861,
     "end_time": "2022-01-21T21:37:17.746756",
     "exception": false,
     "start_time": "2022-01-21T21:37:06.832895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #disable all tensorflow logging output\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "594463d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:37:17.824193Z",
     "iopub.status.busy": "2022-01-21T21:37:17.823494Z",
     "iopub.status.idle": "2022-01-21T21:37:17.832510Z",
     "shell.execute_reply": "2022-01-21T21:37:17.833098Z",
     "shell.execute_reply.started": "2022-01-21T21:21:46.698391Z"
    },
    "papermill": {
     "duration": 0.069451,
     "end_time": "2022-01-21T21:37:17.833275",
     "exception": false,
     "start_time": "2022-01-21T21:37:17.763824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "single strategy\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# USE MULTIPLE GPUS\n",
    "if os.environ[\"CUDA_VISIBLE_DEVICES\"].count(',') == 0:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('single strategy')\n",
    "else:\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print('multiple strategy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d65910",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:37:17.860884Z",
     "iopub.status.busy": "2022-01-21T21:37:17.860388Z",
     "iopub.status.idle": "2022-01-21T21:37:19.945026Z",
     "shell.execute_reply": "2022-01-21T21:37:19.945694Z",
     "shell.execute_reply.started": "2022-01-21T21:21:46.772733Z"
    },
    "papermill": {
     "duration": 2.100837,
     "end_time": "2022-01-21T21:37:19.945913",
     "exception": false,
     "start_time": "2022-01-21T21:37:17.845076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape (144293, 8)\n",
      "discourse types:  ['Lead' 'Position' 'Evidence' 'Claim' 'Concluding Statement'\n",
      " 'Counterclaim' 'Rebuttal']\n",
      "mean len:  1200.791202622442\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "3  When people have phones, they know about certa...       Evidence   \n",
       "4  Driving is one of the way how to get around. P...          Claim   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/feedback-prize-2021/train.csv')\n",
    "print('df shape', train.shape)\n",
    "print('discourse types: ', train['discourse_type'].unique())\n",
    "print('mean len: ', train['discourse_end'].mean())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e47709f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:37:19.997352Z",
     "iopub.status.busy": "2022-01-21T21:37:19.996495Z",
     "iopub.status.idle": "2022-01-21T21:37:20.020967Z",
     "shell.execute_reply": "2022-01-21T21:37:20.021896Z",
     "shell.execute_reply.started": "2022-01-21T21:21:48.465681Z"
    },
    "papermill": {
     "duration": 0.057251,
     "end_time": "2022-01-21T21:37:20.022069",
     "exception": false,
     "start_time": "2022-01-21T21:37:19.964818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions for loading and train/val data\n",
    "\n",
    "def load_train_data(MODEL_NAME=\"bert-base-cased\", MAX_LEN=1024):\n",
    "    # construct tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # load csv file\n",
    "    df = pd.read_csv('../input/feedback-prize-2021/train.csv')\n",
    "    IDS = df.id.unique()\n",
    "    train_ids = np.zeros((len(IDS), MAX_LEN), dtype='int32')\n",
    "    train_attention = np.zeros((len(IDS), MAX_LEN), dtype='int32')\n",
    "    \n",
    "    # init labels\n",
    "    label_to_ind = {\n",
    "        'Lead_b': 0,\n",
    "        'Lead_i': 1,\n",
    "        'Position_b': 2,\n",
    "        'Position_i': 3,\n",
    "        'Evidence_b': 4,\n",
    "        'Evidence_i': 5,\n",
    "        'Claim_b': 6,\n",
    "        'Claim_i': 7,\n",
    "        'Concluding Statement_b': 8,\n",
    "        'Concluding Statement_i': 9,\n",
    "        'Counterclaim_b': 10,\n",
    "        'Counterclaim_i': 11,\n",
    "        'Rebuttal_b': 12,\n",
    "        'Rebuttal_i': 13,\n",
    "        'other': 14\n",
    "    }    \n",
    "    train_labels = np.zeros((len(IDS), MAX_LEN, len(label_to_ind)), dtype='int32')\n",
    "    \n",
    "    # form samples\n",
    "    for i in range(len(IDS)):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        # read txt file\n",
    "        filename = '../input/feedback-prize-2021/train/{}.txt'.format(IDS[i])\n",
    "        txt = open(filename, 'r').read()\n",
    "        \n",
    "        # tokenize\n",
    "        tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                       truncation=True, return_offsets_mapping=True)\n",
    "        train_ids[i, :] = tokens['input_ids']\n",
    "        train_attention[i, :] = tokens['attention_mask']\n",
    "        offsets = tokens['offset_mapping']\n",
    "        \n",
    "        # extract labels for each token\n",
    "        curr_df = df.loc[df.id==IDS[i]]\n",
    "        offset_ind = 0\n",
    "        for index,row in curr_df.iterrows():\n",
    "            label = row.discourse_type + '_b'\n",
    "            \n",
    "            w_start = row.discourse_start\n",
    "            w_end = row.discourse_end\n",
    "            \n",
    "            if offset_ind >= len(offsets):\n",
    "                break\n",
    "            \n",
    "            # set labels\n",
    "            t_start = offsets[offset_ind][0]\n",
    "            while w_end > t_start:\n",
    "                # exit condition\n",
    "                if offset_ind >= len(offsets):\n",
    "                    break\n",
    "                \n",
    "                # get current token index\n",
    "                t_start = offsets[offset_ind][0]\n",
    "                t_end = offsets[offset_ind][1]\n",
    "                \n",
    "                # set label if within range\n",
    "                if t_end <= w_end:\n",
    "                    train_labels[i, offset_ind, label_to_ind[label]] = 1\n",
    "                    label = row.discourse_type + '_i'\n",
    "                \n",
    "                # update global var(s)\n",
    "                offset_ind += 1\n",
    "    train_labels[:, :, 14] = 1 - np.max(train_labels, axis=-1)\n",
    "    return train_ids, train_attention, train_labels\n",
    "\n",
    "def load_test_data(MODEL_NAME=\"bert-base-cased\", MAX_LEN=1024):\n",
    "    # construct tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    IDS = os.listdir('../input/feedback-prize-2021/test')\n",
    "    IDS = [i.split('.')[0] for i in IDS]\n",
    "    test_ids = np.zeros((len(IDS), MAX_LEN), dtype='int32')\n",
    "    test_attention = np.zeros((len(IDS), MAX_LEN), dtype='int32')\n",
    "    \n",
    "    # form samples\n",
    "    for i in range(len(IDS)):\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "        # read txt file\n",
    "        filename = '../input/feedback-prize-2021/test/{}.txt'.format(IDS[i])\n",
    "        txt = open(filename, 'r').read()\n",
    "        \n",
    "        # tokenize\n",
    "        tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                       truncation=True, return_offsets_mapping=True)\n",
    "        test_ids[i, :] = tokens['input_ids']\n",
    "        test_attention[i, :] = tokens['attention_mask']\n",
    "    \n",
    "    return test_ids, test_attention, IDS\n",
    "\n",
    "def train_val(model, ids, attention, labels, \n",
    "              train_size=0.8, \n",
    "              epochs=5,\n",
    "              batch_size=32,\n",
    "              saved_name='saved_model.h5'\n",
    "             ):\n",
    "    # TRAIN VALID SPLIT 80% 20%\n",
    "    np.random.seed(42)\n",
    "    IDS = pd.read_csv('../input/feedback-prize-2021/train.csv').id.unique()\n",
    "    inds = [i for i in range(len(IDS))]\n",
    "    np.random.shuffle(inds)\n",
    "    split_point = int(train_size * len(inds))\n",
    "    train_idx = inds[:split_point]\n",
    "    val_idx = inds[split_point:]\n",
    "    print('Train size',len(train_idx),', Valid size',len(val_idx))\n",
    "\n",
    "    print('start training...')\n",
    "    model.fit(x = [ids[train_idx,], attention[train_idx,]],\n",
    "              y = labels[train_idx,],\n",
    "              validation_data = ([ids[val_idx,], attention[val_idx,]],\n",
    "                                 labels[val_idx,]),\n",
    "              epochs = epochs,\n",
    "              batch_size = batch_size,\n",
    "              verbose = 2)\n",
    "\n",
    "    # SAVE MODEL WEIGHTS\n",
    "    model.save_weights(saved_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "915e3f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:37:20.067831Z",
     "iopub.status.busy": "2022-01-21T21:37:20.067142Z",
     "iopub.status.idle": "2022-01-21T21:37:20.070547Z",
     "shell.execute_reply": "2022-01-21T21:37:20.070100Z",
     "shell.execute_reply.started": "2022-01-21T21:21:48.490076Z"
    },
    "papermill": {
     "duration": 0.030528,
     "end_time": "2022-01-21T21:37:20.070662",
     "exception": false,
     "start_time": "2022-01-21T21:37:20.040134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define models\n",
    "def build_model(MODEL_NAME=\"bert-base-cased\", MAX_LEN=1024, LR=1e-4):\n",
    "    # construct input\n",
    "    input_ids = tf.keras.layers.Input(shape=(MAX_LEN,), name='input_ids', dtype='int32')\n",
    "    mask = tf.keras.layers.Input(shape=(MAX_LEN,), name='attention_mask', dtype='int32')\n",
    "    \n",
    "    # pretrained/finetuned model (Transformers)\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    backbone = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n",
    "#     backbone.trainable = False\n",
    "    \n",
    "#     # save the model\n",
    "#     os.mkdir('model')\n",
    "#     backbone.save_pretrained('model')\n",
    "#     config.save_pretrained('model')\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "#     tokenizer.save_pretrained('model')\n",
    "    \n",
    "    # downstream output layer(s)\n",
    "    x = backbone(input_ids, attention_mask=mask)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x[0])\n",
    "    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    # integration\n",
    "    model = tf.keras.Model(inputs=[input_ids,mask], outputs=x)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = LR),\n",
    "                  loss = [tf.keras.losses.CategoricalCrossentropy()],\n",
    "                  metrics = [tf.keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a64f53f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:37:20.097926Z",
     "iopub.status.busy": "2022-01-21T21:37:20.097369Z",
     "iopub.status.idle": "2022-01-21T21:37:20.100303Z",
     "shell.execute_reply": "2022-01-21T21:37:20.099915Z",
     "shell.execute_reply.started": "2022-01-21T16:44:05.054889Z"
    },
    "papermill": {
     "duration": 0.018241,
     "end_time": "2022-01-21T21:37:20.100404",
     "exception": false,
     "start_time": "2022-01-21T21:37:20.082163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL_NAME = \"bert-base-cased\"\n",
    "# MODEL_NAME = \"../input/feedbacksaved/BERT\" # load from pretrained.\n",
    "# MAX_LEN = 512\n",
    "# LR=0.25e-4\n",
    "\n",
    "# # # MODEL_NAME = 'allenai/longformer-base-4096'\n",
    "# # MODEL_NAME = '../input/feedbacksaved/LongFormer'\n",
    "# # MAX_LEN = 1024\n",
    "# # LR=0.25e-4\n",
    "\n",
    "# # # processing data\n",
    "# # train_ids, train_attention, train_labels = load_train_data(MODEL_NAME=MODEL_NAME, MAX_LEN=MAX_LEN)\n",
    "\n",
    "# # with open('tokenized_data_longformer.pkl', 'wb') as f:\n",
    "# #     saved = {\n",
    "# #         'train_ids': train_ids,\n",
    "# #         'train_attention': train_attention,\n",
    "# #         'train_labels': train_labels\n",
    "# #     }\n",
    "# #     pickle.dump(saved, f)\n",
    "\n",
    "# # load saved data and build model\n",
    "# with open('../input/feedbacksaved/tokenized_data_longformer.pkl', 'rb') as f:\n",
    "#     saved = pickle.load(f)\n",
    "#     ids = saved['train_ids'][:, :MAX_LEN]\n",
    "#     attention = saved['train_attention'][:, :MAX_LEN]\n",
    "#     labels = saved['train_labels'][:, :MAX_LEN, :]\n",
    "\n",
    "# print('input seq shape', ids.shape)\n",
    "# print('attention shape', attention.shape)\n",
    "# print('labels shape', labels.shape)\n",
    "\n",
    "# with strategy.scope():\n",
    "#     model = build_model(MODEL_NAME=MODEL_NAME, MAX_LEN=MAX_LEN, LR=LR)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abd7ec80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:37:20.126838Z",
     "iopub.status.busy": "2022-01-21T21:37:20.126314Z",
     "iopub.status.idle": "2022-01-21T21:37:20.129876Z",
     "shell.execute_reply": "2022-01-21T21:37:20.129450Z",
     "shell.execute_reply.started": "2022-01-21T17:36:06.790558Z"
    },
    "papermill": {
     "duration": 0.018056,
     "end_time": "2022-01-21T21:37:20.129981",
     "exception": false,
     "start_time": "2022-01-21T21:37:20.111925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # load trained model if available\n",
    "# model.load_weights('./BERT_entire.h5')\n",
    "\n",
    "# # # train-val model\n",
    "# # train_val(model, ids, attention, labels, \n",
    "# #           train_size=0.8, \n",
    "# #           epochs=5,\n",
    "# #           batch_size=16,\n",
    "# #           saved_name='saved_model.h5')\n",
    "\n",
    "# # train on entire training set\n",
    "# train_val(model, ids, attention, labels, \n",
    "#           train_size=1.0, \n",
    "#           epochs=3,\n",
    "#           batch_size=4,\n",
    "#           saved_name='{}_entire_0.h5'.format(MODEL_NAME.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5837f00c",
   "metadata": {
    "papermill": {
     "duration": 0.011213,
     "end_time": "2022-01-21T21:37:20.152595",
     "exception": false,
     "start_time": "2022-01-21T21:37:20.141382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"/kaggle/working/LongFormer_entire.h5\"> Download File </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae706e",
   "metadata": {
    "papermill": {
     "duration": 0.011301,
     "end_time": "2022-01-21T21:37:20.175348",
     "exception": false,
     "start_time": "2022-01-21T21:37:20.164047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Make Prediction on Test Set, and Postprocessing**  \n",
    "The following codes are mainly for test-time running and post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a36618",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:37:20.212437Z",
     "iopub.status.busy": "2022-01-21T21:37:20.200803Z",
     "iopub.status.idle": "2022-01-21T21:37:20.243133Z",
     "shell.execute_reply": "2022-01-21T21:37:20.242730Z",
     "shell.execute_reply.started": "2022-01-21T21:26:08.702344Z"
    },
    "papermill": {
     "duration": 0.056262,
     "end_time": "2022-01-21T21:37:20.243235",
     "exception": false,
     "start_time": "2022-01-21T21:37:20.186973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word_to_label(folder, pred, IDS, MODEL_NAME = \"bert-base-cased\", MAX_LEN=1024):\n",
    "    # init labels\n",
    "    label_to_ind = {\n",
    "        'Lead_b': 0,\n",
    "        'Lead_i': 1,\n",
    "        'Position_b': 2,\n",
    "        'Position_i': 3,\n",
    "        'Evidence_b': 4,\n",
    "        'Evidence_i': 5,\n",
    "        'Claim_b': 6,\n",
    "        'Claim_i': 7,\n",
    "        'Concluding Statement_b': 8,\n",
    "        'Concluding Statement_i': 9,\n",
    "        'Counterclaim_b': 10,\n",
    "        'Counterclaim_i': 11,\n",
    "        'Rebuttal_b': 12,\n",
    "        'Rebuttal_i': 13,\n",
    "        'other': 14\n",
    "    }\n",
    "    ind_to_label = dict()\n",
    "    for key in label_to_ind:\n",
    "        ind_to_label[label_to_ind[key]] = key\n",
    "        \n",
    "    # construct tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    word_classes = {\n",
    "        'id': list(),\n",
    "        'classes': list()\n",
    "    }\n",
    "    for i in range(len(IDS)):\n",
    "        word_classes['id'].append(IDS[i])\n",
    "        \n",
    "        # verbose\n",
    "        if i % 1000 == 0:\n",
    "            print(i)\n",
    "            \n",
    "        # read txt file\n",
    "        filename = '../input/feedback-prize-2021/{}/{}.txt'.format(folder, IDS[i])\n",
    "        txt = open(filename, 'r').read()\n",
    "        txt_len = len(txt)\n",
    "        words_num = len(txt.split())\n",
    "        \n",
    "        # tokenize\n",
    "        tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n",
    "                                       truncation=True, return_offsets_mapping=True)\n",
    "        token_ids = tokens['input_ids']\n",
    "        offsets = tokens['offset_mapping']\n",
    "        \n",
    "        # extract word class\n",
    "        word_class = list() # 1D array\n",
    "        curr_words = list() # list of tuple (token_len, label_name)\n",
    "        for j in range(MAX_LEN):\n",
    "            # exit condition\n",
    "            if len(word_class) == words_num:\n",
    "                break\n",
    "            \n",
    "            # get current token index\n",
    "            t_start = offsets[j][0]\n",
    "            t_end = offsets[j][1]\n",
    "            \n",
    "            token_len = t_end - t_start\n",
    "            curr_label = ind_to_label[pred[i, j]]\n",
    "            curr_words.append((token_len, curr_label))\n",
    "            \n",
    "            # update word map                \n",
    "            if t_end >= txt_len or txt[t_end] == ' ': # means the ending of a word\n",
    "                if len(curr_words) < 2: # the word is not splitted\n",
    "                    word_class.append(curr_label)\n",
    "                else:\n",
    "                    word_class.append(sorted(curr_words, key=lambda x: x[0])[-1][1])\n",
    "                curr_words = list()\n",
    "        word_classes['classes'].append(word_class)\n",
    "        \n",
    "    return word_classes\n",
    "\n",
    "def form_raw_df(word_classes):\n",
    "    res = {\n",
    "        'id': list(),\n",
    "        'class': list(),\n",
    "        'predictionstring': list(), # 2D array\n",
    "    }\n",
    "    \n",
    "    for i in range(len(word_classes['id'])):\n",
    "        curr_seg = list() # (class, idx)\n",
    "        for j in range(len(word_classes['classes'][i])):\n",
    "            curr_class = word_classes['classes'][i][j].split('_')\n",
    "            class_name = curr_class[0]\n",
    "            pos = curr_class[1] if len(curr_class) > 1 else 'i'\n",
    "            \n",
    "#             if len(curr_seg) < 1 or class_name == curr_seg[-1][0]:\n",
    "#                 curr_seg.append((class_name, j))\n",
    "#             else:\n",
    "#                 res['id'].append(word_classes['id'][i])\n",
    "#                 res['class'].append(curr_seg[-1][0])\n",
    "#                 res['predictionstring'].append(' '.join([str(k[1]) for k in curr_seg]))\n",
    "#                 curr_seg = [(class_name, j)]\n",
    "\n",
    "            if pos == 'b': # if it's the begining of a segment\n",
    "                if len(curr_seg) < 1: # haven't init\n",
    "                    curr_seg.append((class_name, j))\n",
    "                else: # the close of previous segment\n",
    "                    res['id'].append(word_classes['id'][i])\n",
    "                    res['class'].append(curr_seg[-1][0])\n",
    "                    res['predictionstring'].append(' '.join([str(k[1]) for k in curr_seg]))\n",
    "                    curr_seg = [(class_name, j)]\n",
    "            elif len(curr_seg) >= 1 and class_name == curr_seg[-1][0]: # if it's the inside of a segment with same class as begining\n",
    "                curr_seg.append((class_name, j))\n",
    "            elif len(curr_seg) >= 1:\n",
    "                res['id'].append(word_classes['id'][i])\n",
    "                res['class'].append(curr_seg[-1][0])\n",
    "                res['predictionstring'].append(' '.join([str(k[1]) for k in curr_seg]))\n",
    "                curr_seg = list()\n",
    "            else:\n",
    "                curr_seg = list()\n",
    "    \n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "def post_processing_mode(folder, word_classes): # determine the class of a sentence by mode of label\n",
    "    res = {\n",
    "        'id': list(),\n",
    "        'class': list(),\n",
    "        'predictionstring': list(), # 2D array\n",
    "    }\n",
    "    \n",
    "    def find_mode_label(arr):\n",
    "        label_ct = dict()\n",
    "        for label in arr:\n",
    "            if label_ct.get(label) == None:\n",
    "                label_ct[label] = 1\n",
    "            else:\n",
    "                label_ct[label] += 1\n",
    "        label = sorted(label_ct.items(), key=lambda x: x[1])[-1][0]\n",
    "        return label\n",
    "    \n",
    "    for i in range(len(word_classes['id'])):\n",
    "        # read txt file\n",
    "        filename = '../input/feedback-prize-2021/{}/{}.txt'.format(folder, word_classes['id'][i])\n",
    "        words = open(filename, 'r').read().split()\n",
    "        \n",
    "        ending = ['.', '!', '?']\n",
    "        curr_sentence = list() # list of tuples: (word_idx, class_name)\n",
    "        for j in range(len(word_classes['classes'][i])):\n",
    "            curr_sentence.append((j, word_classes['classes'][i][j]))\n",
    "            word = words[j]\n",
    "            if word[-1] in ending:\n",
    "                label = find_mode_label([k[1].split('_')[0] for k in curr_sentence])\n",
    "                \n",
    "                if len(res['id']) > 0 and word_classes['id'][i] == res['id'][-1] and label == res['class'][-1]:\n",
    "                    res['predictionstring'][-1] += ' ' + ' '.join([str(k[0]) for k in curr_sentence])\n",
    "                else:\n",
    "                    res['id'].append(word_classes['id'][i])\n",
    "                    res['class'].append(label)\n",
    "                    res['predictionstring'].append(' '.join([str(k[0]) for k in curr_sentence]))\n",
    "                \n",
    "                # clear up\n",
    "                curr_sentence = list()\n",
    "                \n",
    "        if len(curr_sentence) > 0:\n",
    "            label = find_mode_label([k[1].split('_')[0] for k in curr_sentence])\n",
    "            \n",
    "            if len(res['id']) > 0 and word_classes['id'][i] == res['id'][-1] and label == res['class'][-1]:\n",
    "                res['predictionstring'][-1] += ' ' + ' '.join([str(k[0]) for k in curr_sentence])\n",
    "            else:\n",
    "                res['id'].append(word_classes['id'][i])\n",
    "                res['class'].append(label)\n",
    "                res['predictionstring'].append(' '.join([str(k[0]) for k in curr_sentence]))\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "# =====================================================================\n",
    "target_map_rev = {0: 'Lead', 1: 'Position', 2: 'Evidence', 3: 'Claim', 4: 'Concluding Statement', 5: 'Counterclaim', 6: 'Rebuttal', 7: 'blank'}\n",
    "\n",
    "def get_preds(dataset = 'train', verbose = True, text_ids = None, preds = None):\n",
    "    # construct tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    all_predictions = list()\n",
    "    for id_num in range(len(preds)):\n",
    "#         if (id_num % 100 == 0) & (verbose): print(id_num, ', ', end = '')\n",
    "        n = text_ids[id_num]\n",
    "        name = f'../input/feedback-prize-2021/{dataset}/{n}.txt'\n",
    "        txt = open(name, 'r').read()\n",
    "        tokens = tokenizer.encode_plus(txt, max_length = MAX_LEN, padding = 'max_length', truncation = True, return_offsets_mapping = True)\n",
    "        off = tokens['offset_mapping']\n",
    "        w = list()\n",
    "        blank = True\n",
    "        for i in range(len(txt)):\n",
    "            if (txt[i] != ' ') & (txt[i] != '\\n') & (blank == True):\n",
    "                w.append(i)\n",
    "                blank = False\n",
    "            elif (txt[i] == ' ') | (txt[i] == '\\n'):\n",
    "                blank = True\n",
    "        w.append(1e6)\n",
    "        word_map = -1 * np.ones(MAX_LEN, dtype = 'int32')\n",
    "        w_i = 0\n",
    "        for i in range(len(off)):\n",
    "            if off[i][1] == 0: continue\n",
    "            while off[i][0] >= w[w_i + 1]: w_i += 1\n",
    "            word_map[i] = int(w_i)\n",
    "        \n",
    "        pred = preds[id_num,] / 2.0\n",
    "        i = 0\n",
    "        while i < MAX_LEN:\n",
    "            prediction = []\n",
    "            start = pred[i]\n",
    "            if start in [0, 1, 2, 3, 4, 5, 6, 7]:\n",
    "                prediction.append(word_map[i])\n",
    "                i += 1\n",
    "                if i >= MAX_LEN: break\n",
    "                while pred[i] == start + 0.5:\n",
    "                    if not word_map[i] in prediction: prediction.append(word_map[i])\n",
    "                    i += 1\n",
    "                    if i >= MAX_LEN: break\n",
    "            else: i += 1\n",
    "            prediction = [x for x in prediction if x != -1]\n",
    "            if len(prediction) > 4: \n",
    "                all_predictions.append((n, target_map_rev[int(start)], ' '.join([str(x) for x in prediction])))\n",
    "\n",
    "    # MAKE DATAFRAME\n",
    "    df = pd.DataFrame(all_predictions)\n",
    "    df.columns = ['id', 'class', 'predictionstring']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a7c6d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:37:20.271982Z",
     "iopub.status.busy": "2022-01-21T21:37:20.271070Z",
     "iopub.status.idle": "2022-01-21T21:38:14.507771Z",
     "shell.execute_reply": "2022-01-21T21:38:14.505974Z",
     "shell.execute_reply.started": "2022-01-21T21:23:24.363660Z"
    },
    "papermill": {
     "duration": 54.253293,
     "end_time": "2022-01-21T21:38:14.507991",
     "exception": false,
     "start_time": "2022-01-21T21:37:20.254698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFLongformerModel.\n",
      "\n",
      "All the layers of TFLongformerModel were initialized from the model checkpoint at ../input/feedbacksaved/LongFormer.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFLongformerModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loading Complete.\n",
      "0\n",
      "Test Data Loading Complete.\n",
      "2/2 - 16s\n",
      "Prediction Complete.\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"bert-base-cased\"\n",
    "# MODEL_NAME = \"../input/feedbacksaved/BERT\" # load from pretrained.\n",
    "# MAX_LEN = 512\n",
    "\n",
    "# MODEL_NAME = 'allenai/longformer-base-4096'\n",
    "MODEL_NAME = '../input/feedbacksaved/LongFormer'\n",
    "MAX_LEN = 1024\n",
    "\n",
    "# build and load model\n",
    "with strategy.scope():\n",
    "    model = build_model(MODEL_NAME=MODEL_NAME, MAX_LEN=MAX_LEN)\n",
    "model.load_weights('../input/feedbacksaved/LongFormer_entire.h5')\n",
    "print('Model Loading Complete.')\n",
    "\n",
    "# load test data\n",
    "test_ids, test_attention, test_IDS = load_test_data(MODEL_NAME=MODEL_NAME, MAX_LEN=MAX_LEN)\n",
    "print('Test Data Loading Complete.')\n",
    "\n",
    "# make prediction\n",
    "test_pred = model.predict([test_ids, test_attention], batch_size=4, verbose=2).argmax(axis=-1)\n",
    "print('Prediction Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "914acbb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:38:14.567192Z",
     "iopub.status.busy": "2022-01-21T21:38:14.566441Z",
     "iopub.status.idle": "2022-01-21T21:38:14.772911Z",
     "shell.execute_reply": "2022-01-21T21:38:14.772199Z",
     "shell.execute_reply.started": "2022-01-21T21:27:46.336801Z"
    },
    "papermill": {
     "duration": 0.24284,
     "end_time": "2022-01-21T21:38:14.773082",
     "exception": false,
     "start_time": "2022-01-21T21:38:14.530242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Word-Class Mapping Complete.\n"
     ]
    }
   ],
   "source": [
    "# form word-class map\n",
    "word_label_map = word_to_label('test', test_pred, test_IDS, MODEL_NAME=MODEL_NAME, MAX_LEN=MAX_LEN)\n",
    "print('Word-Class Mapping Complete.')\n",
    "\n",
    "# post processing\n",
    "# raw_df = form_raw_df(word_label_map)\n",
    "# test_res_int = raw_df\n",
    "# test_res_int = post_processing_mode('test', word_label_map)\n",
    "\n",
    "# # quick check\n",
    "# print(test_res_int.shape)\n",
    "# test_res_int.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d552e392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:38:14.826271Z",
     "iopub.status.busy": "2022-01-21T21:38:14.825373Z",
     "iopub.status.idle": "2022-01-21T21:38:15.081330Z",
     "shell.execute_reply": "2022-01-21T21:38:15.082034Z",
     "shell.execute_reply.started": "2022-01-21T21:35:38.766518Z"
    },
    "papermill": {
     "duration": 0.285433,
     "end_time": "2022-01-21T21:38:15.082199",
     "exception": false,
     "start_time": "2022-01-21T21:38:14.796766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Lead</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Position</td>\n",
       "      <td>41 42 43 44 45 46 47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>65 66 67 68 69 70 71 72 73 74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>120 121 122 123 124 125 126 127 128 129 130 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Claim</td>\n",
       "      <td>314 315 316 317 318 319 320 321 322 323 324 32...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>342 343 344 345 346 347 348 349 350 351 352 35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>Concluding Statement</td>\n",
       "      <td>560 561 562 563 564 565 566 567 568 569 570 57...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Lead</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Position</td>\n",
       "      <td>51 52 53 54 55 56 57 58 59 60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                 class  \\\n",
       "0  0FB0700DAF44                  Lead   \n",
       "1  0FB0700DAF44              Position   \n",
       "2  0FB0700DAF44                 Claim   \n",
       "3  0FB0700DAF44                 Claim   \n",
       "4  0FB0700DAF44              Evidence   \n",
       "5  0FB0700DAF44                 Claim   \n",
       "6  0FB0700DAF44              Evidence   \n",
       "7  0FB0700DAF44  Concluding Statement   \n",
       "8  D72CB1C11673                  Lead   \n",
       "9  D72CB1C11673              Position   \n",
       "\n",
       "                                    predictionstring  \n",
       "0  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
       "1                               41 42 43 44 45 46 47  \n",
       "2    49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  \n",
       "3                      65 66 67 68 69 70 71 72 73 74  \n",
       "4  120 121 122 123 124 125 126 127 128 129 130 13...  \n",
       "5  314 315 316 317 318 319 320 321 322 323 324 32...  \n",
       "6  342 343 344 345 346 347 348 349 350 351 352 35...  \n",
       "7  560 561 562 563 564 565 566 567 568 569 570 57...  \n",
       "8  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
       "9                      51 52 53 54 55 56 57 58 59 60  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res_int = get_preds(dataset='test', verbose=False, text_ids=test_IDS, preds=test_pred)\n",
    "# map_clip = {'Lead':9, 'Position':5, 'Evidence':14, 'Claim':3, 'Concluding Statement':11, 'Counterclaim':6, 'Rebuttal':4}\n",
    "# def threshold(df):\n",
    "#     df = df.copy()\n",
    "#     for key, value in map_clip.items():\n",
    "#     # if df.loc[df['class']==key,'len'] < value \n",
    "#         index = df.loc[df['class']==key].query(f'len<{value}').index\n",
    "#         df.drop(index, inplace = True)\n",
    "#     return df\n",
    "\n",
    "# test_res_int['len'] = test_res_int['predictionstring'].apply(lambda x:len(x.split()))\n",
    "# test_res_int = threshold(test_res_int)\n",
    "\n",
    "# quick check\n",
    "print(test_res_int.shape)\n",
    "test_res_int.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "462dfe78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-21T21:38:15.114075Z",
     "iopub.status.busy": "2022-01-21T21:38:15.113367Z",
     "iopub.status.idle": "2022-01-21T21:38:15.119235Z",
     "shell.execute_reply": "2022-01-21T21:38:15.118830Z",
     "shell.execute_reply.started": "2022-01-21T21:35:47.779396Z"
    },
    "papermill": {
     "duration": 0.023071,
     "end_time": "2022-01-21T21:38:15.119342",
     "exception": false,
     "start_time": "2022-01-21T21:38:15.096271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write to file\n",
    "test_res_int.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 79.339455,
   "end_time": "2022-01-21T21:38:18.115316",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-21T21:36:58.775861",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
